{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 7701\n",
      "               name                                        top_5_words\n",
      "0     0504Traveller  [usatoday, http, southwestair, p8vcz4xthm, 6rl...\n",
      "1          09202010               [baggages, 699, rdu, la, vu5lbzxtrx]\n",
      "2      0veranalyser       [runways, flightled, flights, note, showing]\n",
      "3           0xjared            [vibe, depends, fair, getting, jetblue]\n",
      "4           10Eshaa        [lmfaooooo, 4llwi5oxvo, hook, fleek, fleet]\n",
      "...             ...                                                ...\n",
      "7696  zombiesausage             [sms, damn, impressed, worth, updates]\n",
      "7697      zozo24dad           [2413, la, delayed, americanair, flight]\n",
      "7698       zsalim03      [1hr, aa2450, arpt, vanessaannz, inclemental]\n",
      "7699       zslick99  [ticketing, unmanned, unorganized, stations, c...\n",
      "7700      zupshawrl  [luv, feltthelove, television, commercials, tr...\n",
      "\n",
      "[7701 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Find the number of unique users\n",
    "unique_users = data['name'].nunique()\n",
    "print(f'Number of unique users: {unique_users}')\n",
    "\n",
    "# Compute top-5 words from their tweets using TF-IDF approach\n",
    "def get_top_n_words(tfidf_matrix, feature_names, n=5):\n",
    "    top_n_words = []\n",
    "    for row in tfidf_matrix:\n",
    "        sorted_indices = np.argsort(row)[::-1][:n]\n",
    "        top_n_words.append([feature_names[i] for i in sorted_indices])\n",
    "    return top_n_words\n",
    "\n",
    "# Group tweets by user\n",
    "grouped_tweets = data.groupby('name')['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Apply TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(grouped_tweets['text']).toarray()\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get top-5 words for each user\n",
    "grouped_tweets['top_5_words'] = get_top_n_words(tfidf_matrix, feature_names)\n",
    "print(grouped_tweets[['name', 'top_5_words']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            airline     name  \\\n",
      "0          American  otisday   \n",
      "1          American  otisday   \n",
      "2          American  otisday   \n",
      "3          American  otisday   \n",
      "4          American  otisday   \n",
      "..              ...      ...   \n",
      "165  Virgin America  wmrrock   \n",
      "166  Virgin America  wmrrock   \n",
      "167  Virgin America  wmrrock   \n",
      "168  Virgin America  wmrrock   \n",
      "169  Virgin America  wmrrock   \n",
      "\n",
      "                                                  text tweet_location  \\\n",
      "0    @AmericanAir @praywinn between understaffing, ...          Pekin   \n",
      "1    @AmericanAir @jameswester see, James, we only ...          Pekin   \n",
      "2    @AmericanAir @cjdjpdx not a valid response in ...          Pekin   \n",
      "3    @AmericanAir @kaps12 this is an international ...          Pekin   \n",
      "4    @AmericanAir @PatrichRuben no, profit maximiza...          Pekin   \n",
      "..                                                 ...            ...   \n",
      "165  @VirginAmerica on flight VX399 headed to LA fr...             CT   \n",
      "166  @VirginAmerica You should still develop an app...             CT   \n",
      "167           @VirginAmerica got it. All set - Thanks!             CT   \n",
      "168  @VirginAmerica Only thing I see on passbook is...             CT   \n",
      "169  @VirginAmerica how come you don't have an iPho...             CT   \n",
      "\n",
      "    airline_sentiment  \n",
      "0            negative  \n",
      "1            negative  \n",
      "2            negative  \n",
      "3            negative  \n",
      "4            negative  \n",
      "..                ...  \n",
      "165          negative  \n",
      "166           neutral  \n",
      "167          positive  \n",
      "168           neutral  \n",
      "169          negative  \n",
      "\n",
      "[170 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the most active user for each airline\n",
    "most_active_users = data.groupby('airline')['name'].agg(lambda x: x.value_counts().idxmax()).reset_index()\n",
    "\n",
    "# Merge with the original data to get their tweets, tweeting location, and tweet sentiment\n",
    "most_active_users_data = pd.merge(most_active_users, data, on=['airline', 'name'], how='left')\n",
    "\n",
    "# Select relevant columns\n",
    "most_active_users_data = most_active_users_data[['airline', 'name', 'text', 'tweet_location', 'airline_sentiment']]\n",
    "\n",
    "print(most_active_users_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in tweet_location: 4733\n",
      "Missing values in user_timezone: 4820\n",
      "Number of rows after dropping missing values: 7758\n"
     ]
    }
   ],
   "source": [
    "# Find the number of missing values for tweet_location and user_timezone\n",
    "missing_tweet_location = data['tweet_location'].isna().sum()\n",
    "missing_user_timezone = data['user_timezone'].isna().sum()\n",
    "\n",
    "print(f'Missing values in tweet_location: {missing_tweet_location}')\n",
    "print(f'Missing values in user_timezone: {missing_user_timezone}')\n",
    "\n",
    "# Drop rows with missing values in tweet_location and user_timezone\n",
    "data_cleaned = data.dropna(subset=['tweet_location', 'user_timezone'])\n",
    "\n",
    "print(f'Number of rows after dropping missing values: {data_cleaned.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC-08:00]\n",
      "0   2015-02-24 11:35:52-08:00\n",
      "1   2015-02-24 11:15:59-08:00\n",
      "2   2015-02-24 11:15:48-08:00\n",
      "3   2015-02-24 11:15:36-08:00\n",
      "4   2015-02-24 11:14:45-08:00\n",
      "Name: tweet_created, dtype: datetime64[ns, UTC-08:00]\n"
     ]
    }
   ],
   "source": [
    "# Parse the tweet_created field as datetime\n",
    "data['tweet_created'] = pd.to_datetime(data['tweet_created'])\n",
    "\n",
    "# Verify the changes\n",
    "print(data['tweet_created'].dtypes)\n",
    "print(data['tweet_created'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets from Philadelphia: 92\n",
      "Different spellings of Philadelphia: ['Philadelphia, Pa' 'Philly Burbs' 'Los Angeles, CA (via Philly)'\n",
      " 'Phila. PA' 'Philadelphia, PA' 'Philadelphia PA ' 'Philadelphia PA'\n",
      " 'Philly' 'Philadelphia/Cali' 'Philadelphia' 'philadephia, pa'\n",
      " 'Philadelphia, PA USA' 'Philly Yo' 'Philly Area'\n",
      " 'Philly, Chicago, MSP, Vegas' 'Philly to NY/NJ' 'Phila, Princeton, NYC. '\n",
      " 'Old City Philly' 'philadelphia, pa' 'Los Angeles by way of Philly'\n",
      " 'Philadelphia, USA' 'philadelphia' 'Philadelphia Suburbs' 'Phila, PA']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a list of possible variations of \"Philadelphia\"\n",
    "philadelphia_variations = [\n",
    "    r'philadelphia', r'philly', r'phila', r'phillydelphia', r'phillydelphia', r'phillydelphia', r'phillydelphia'\n",
    "]\n",
    "\n",
    "# Create a regex pattern to match any of the variations\n",
    "pattern = re.compile('|'.join(philadelphia_variations), re.IGNORECASE)\n",
    "\n",
    "# Find all tweets from Philadelphia\n",
    "philadelphia_tweets = data[data['tweet_location'].str.contains(pattern, na=False)]\n",
    "\n",
    "# Print the total number of tweets from Philadelphia\n",
    "total_philadelphia_tweets = philadelphia_tweets.shape[0]\n",
    "print(f'Total number of tweets from Philadelphia: {total_philadelphia_tweets}')\n",
    "\n",
    "# Print all different spellings of Philadelphia found in the dataset\n",
    "unique_spellings = philadelphia_tweets['tweet_location'].unique()\n",
    "print(f'Different spellings of Philadelphia: {unique_spellings}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the dataset\n",
    "subset_data = data[data['airline_sentiment_confidence'] > 0.6]\n",
    "\n",
    "# Save the subset to a CSV file\n",
    "subset_data.to_csv('subset_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
